{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2b2e98-8184-4c5b-9ca9-a528401200cc",
   "metadata": {},
   "source": [
    "As per the notebook `transformers-finetuned.ipynb`, we have finetuned a transformer model and saved it locally, as well as pushed it to the Huggingface Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8964a3b5-80a4-4b46-a556-69a0eb9a9569",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adb5e5b-dc42-4007-916e-8c1bcad35120",
   "metadata": {},
   "source": [
    "Retrieve our model from the Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e964209-0d33-4f6a-b877-2901b7305b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2024.11.10: Fast Mistral patching. Transformers:4.46.3.\n",
      "   \\\\   /|    GPU: NVIDIA H100 PCIe. Max memory: 79.097 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model (Unsloth likely returns a tuple, so unpack it)\n",
    "model, metadata = FastLanguageModel.from_pretrained(\"slindgren/climate-mistral\")\n",
    "# Prepare the model for inference\n",
    "FastLanguageModel.for_inference(model)\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"slindgren/climate-mistral\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "98d33214-f1b5-43c7-a808-d94bcc3d986f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" {'topicalcontent': 'Climate change is a debated issue in modern politics. Some advocate for immediate and strong governmental intervention, while others prioritize economic growth over environmental concerns. Public sentiment is divided, with protests and activism on one side and climate skepticism on the other.',  'politicalideologystance': 'This text discusses the political debate surrounding climate change, with mentions of those advocating for governmental intervention and those prioritizing economic growth.',  'sentimenttone': 'The sentiment tone of this text is neutral, as it discusses the debate and division surrounding climate change without expressing a clear positive or negative opinion.'}\""
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prompt\n",
    "\n",
    "task = (\n",
    "    \"You are a text analysis machine. Respond only in a valid Python dictionary format. \"\n",
    "    \"Use the following keys: 'topical-content', 'political-ideology-stance', and 'sentiment-tone'. \"\n",
    "    \"All keys must have as their value full text descriptive sentences in the format of a python string. \"\n",
    "    \"Do not include any additional text or comments. Only output the dictionary. \"\n",
    "    \"Here is the text to analyze:\\n\"\n",
    ")\n",
    "\n",
    "# Input text\n",
    "input_text = \"Climate change is one of the most debated issues in modern politics. While some advocate for immediate and strong governmental intervention, others prioritize economic growth over environmental concerns. Public sentiment appears divided, with protests and activism on one side and climate skepticism on the other.\"\n",
    "\n",
    "# Create a prompt to guide the model\n",
    "prompt = (\n",
    "    \"\"\n",
    "    f\"{task}\"\n",
    "    f\"{input_text}\"\n",
    ")\n",
    "\n",
    "# Tokenize the input\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "# Generate the model's response\n",
    "outputs = model.generate(\n",
    "    **inputs, \n",
    "    max_length=2000, \n",
    "    temperature=0.2,  # Lower for more focused responses\n",
    "    top_p=0.9,        # Narrow the sampling range\n",
    ")\n",
    "\n",
    "\n",
    "full_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "output = full_output.replace(task,\"\")\n",
    "output = output.replace(input_text,\"\")\n",
    "output = output.replace(\"\\n\",\" \")\n",
    "output = output.replace(\"-\",\"\")\n",
    "\n",
    "\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19bcffa-74ed-46a0-a44a-6d8b90c20914",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PARSE THE OUTPUT INTO A REAL DICT"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
